{\rtf1\ansi\ansicpg1252\cocoartf2757
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;\f2\fnil\fcharset0 HelveticaNeue;
\f3\fnil\fcharset0 HelveticaNeue-Bold;\f4\fnil\fcharset0 HelveticaNeue-Italic;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww17420\viewh14220\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b\fs36 \cf0 README describing technical methods \
\
Command line arguments (configurations): 
\f1\b0\fs24 \
\

\f0\b Layers: \
\
\pard\pardeftab560\slleading20\partightenfactor0

\f2\b0 \cf0 python3 -m memray run  train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=8 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0
\fs26 \
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 Heads: \
\
\pard\pardeftab560\slleading20\partightenfactor0

\f2\b0 \cf0 python3 -m memray run  train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=8 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0
\fs26 \
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 Embeddings: \
\
\pard\pardeftab560\slleading20\partightenfactor0

\f2\b0 \cf0 python3 -m memray run  train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=256 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0
\fs26 \
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf0 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\b\fs36 \cf0 Optimizations: 
\fs30 \

\f1\b0 Modified train.py file to achieve certain optimizations, changed files included
\f0\b  
\f2\b0\fs26 \
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf0 \
\

\f3\b Batch Size reduction: (batch size at 6)\
Used \
\pard\pardeftab560\slleading20\partightenfactor0

\f2\b0\fs24 \cf0 python3 -m memray run  train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=6 --n_layer=8 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0\
\

\f4\i\fs26 Changed in baseline, file also included 
\f2\i0 \
\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0

\f3\b \cf0 Gradient Accumulation to create micro batches:\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0

\f4\i\b0 \cf0 See file\

\f2\i0 \
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0

\f3\b \cf0 SGD Learner Optimizer:\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0

\f4\i\b0 \cf0 See file\

\f2\i0 \
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0

\f3\b \cf0 Mixed Precision:
\f2\b0 \

\f4\i See file
\f2\i0 \
\

\f3\b Full Precision bits:
\f2\b0 \

\f4\i See file 
\f2\i0  }